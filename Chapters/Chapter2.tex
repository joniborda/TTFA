\chapter{Introducción específica} % Main chapter title

\label{Chapter2}

En este capítulo se presentan los requerimientos más importantes, los modelos de inteligencia artificial utilizados y las herramientos usadas para el tratamiento de datos.

\section{Requerimientos}

En esta sección se presentan algunos requerimientos establecidos para el trabajo. En cada requerimiento se detalla su objetivo y alcance, con el fin de precisar las condiciones necesarias para el desarrollo y la correcta implementación del modelo propuesto.


	\begin{itemize}
		\item Importación de datos de ventas desde Shopify: el sistema debe permitir la obtención de datos históricos y actualizados de ventas desde la plataforma Shopify, con el objetivo de utilizarlos en el proceso de entrenamiento del modelo de predicción de demanda.
		\item Importación de datos de inversión publicitaria desde Triple Whale: el sistema debe permitir la incorporación de datos de inversión publicitaria provenientes de la plataforma Triple Whale, con el propósito de integrar esta variable en el análisis y modelado de ventas.
		\item Consolidación de datos provenientes de Shopify y Triple Whale: el sistema debe unificar la información de ventas y de inversión publicitaria en un único conjunto de datos, con el fin de preparar una base consistente para el entrenamiento del modelo de predicción.
		\item Entrenamiento del modelo de predicción de ventas: el objetivo de este requerimiento fue desarrollar y entrenar un modelo de predicción que permita anticipar el comportamiento de las ventas futuras utilizando como base el \textit{dataset} consolidado.
		\item Visualización de predicciones de ventas en Inventory Tracker: el objetivo de este requerimiento fue permitir a los usuarios de Inventory Tracker consultar y analizar las predicciones de ventas generadas por el modelo, integrándolas con los datos históricos disponibles para facilitar la planificación de la producción.

		\item Descarga de predicciones de ventas en formato CSV: el objetivo de este requerimiento fue permitir a los usuarios de Inventory Tracker exportar las predicciones de ventas a un archivo CSV para su análisis externo o integración con otras herramientas de planificación.
	\end{itemize}




\section{Modelos de inteligencia artificial utilizados}

Para abordar el trabajo se evaluaron y aplicaron diversos modelos de inteligencia artificial, dado que el comportamiento de la demanda no responde a un único patrón fijo ni puede ser capturado adecuadamente por un solo tipo de modelo. También, se consideró seleccionar aquel que ofreciera el mejor equilibrio entre precisión, interpretabilidad y capacidad de generalización.

A continuación, se describen los tipos de modelos aplicados:

	\begin{itemize}
		\item \textit{Random Forest Regressor}: se utilizó por su robustez frente a sobreajuste y su capacidad para capturar relaciones no lineales entre variables. Este modelo resulta especialmente útil cuando se integran múltiples fuentes de datos, como ventas históricas e inversión publicitaria.

		\item \textit{Gradient Boosting Machines} (XGBoost, LightGBM): se evaluaron por su alto rendimiento en competencias de ciencia de datos y su eficiencia computacional. Estos modelos permiten optimizar la función de pérdida de manera iterativa, mejorando la precisión en la predicción de series temporales con características externas.

	\item SARIMA (\textit{Seasonal Autoregressive Integrated Moving Average}): se aplicó para modelar la estacionalidad y tendencia presentes en los datos de ventas. SARIMA es especialmente efectivo cuando el comportamiento temporal es predominante y estable a lo largo del tiempo.

	\item \textit{Exponential Smoothing} (ETS): se consideró como alternativa para capturar patrones estacionales y de tendencia con un enfoque más intuitivo y menos paramétrico que SARIMA.

	\item LSTM (\textit{Long Short-Term Memory}): se implementaron redes LSTM debido a su habilidad para retener información a largo plazo y modelar dependencias temporales complejas. Este tipo de red es adecuado para series temporales con patrones no lineales y múltiples variables exógenas, como la inversión publicitaria por plataforma.

	\item GRU (\textit{Gated Recurrent Unit}): se evaluó como una variante más simple y computacionalmente eficiente de las LSTM, útil cuando los recursos de entrenamiento son limitados.

	\item Enfoque híbrido: se exploró la combinación de modelos clásicos (como SARIMA) con redes neuronales (como LSTM), con el fin de aprovechar la capacidad de los primeros para modelar componentes estacionales y de tendencia, y la flexibilidad de las segundas para capturar relaciones no lineales y efectos de variables externas.

	\item Prophet: se incluyó el modelo Prophet, desarrollado por Meta, por su facilidad de uso y capacidad para manejar estacionalidades múltiples, días festivos y cambios en la tendencia. Este modelo resultó de interés para validar la estructura temporal de los datos antes de aplicar modelos más complejos.
	\end{itemize}

Cada uno de estos modelos fue entrenado y validado utilizando el conjunto de datos consolidado de ventas e inversión publicitaria, y se compararon mediante métricas como MAE (Error Absoluto Medio), RMSE (Raíz del Error Cuadrático Medio) y MAPE (Error Porcentual Absoluto Medio), con el fin de seleccionar el que mejor se adaptara a las necesidades de pronóstico de Latech.


\section{Tratamiento de datos (herramientas)}

En este a sección se describen los procesos y herramientas utilizadas para la limpieza, transformación y análisis de los datos, así como los recursos externos desarrollados por terceros que resultaron fundamentales para la obtención y procesamiento de la información.




Bibliotecas para procesamiento y análisis de datos
\begin{itemize}
		\item Pandas: para la manipulación y transformación de datos tabulares, incluyendo operaciones de filtrado, agrupamiento y combinación de datasets.

	\item NumPy: utilizado para operaciones numéricas y manejo eficiente de arreglos multidimensionales.

	\item Scikit-learn: empleada para el preprocesamiento de datos (escalado, codificación de variables categóricas) y la implementación de modelos clásicos de machine learning.

\end{itemize}


Bibliotecas para visualización

\begin{itemize}
		\item Matplotlib y Seaborn: para la generación de gráficos estáticos que facilitaron la identificación de patrones, tendencias y valores atípicos.

\item Plotly: utilizado para la creación de visualizaciones interactivas dentro de los notebooks.
\end{itemize}


Plataformas y APIs externas

\begin{itemize}
		\item Shopify API: para la extracción automatizada de datos históricos de ventas, productos y transacciones.

\item Triple Whale API: para la obtención de datos de inversión publicitaria desglosados por plataforma y campaña.

\item PostgreSQL: sistema de gestión de bases de datos utilizado para el almacenamiento estructurado de los datos consolidados.
\end{itemize}