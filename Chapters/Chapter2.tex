\chapter{Introducción específica} % Main chapter title

\label{Chapter2}

En este capítulo se presentan los requerimientos más importantes, los modelos de inteligencia artificial utilizados y las herramientas usadas para el tratamiento de datos.

\section{Requerimientos}

En esta sección se presentan algunos requerimientos establecidos para el trabajo. En cada requerimiento se detalla su objetivo y alcance, con el fin de precisar las condiciones necesarias para el desarrollo y la correcta implementación del modelo propuesto.


	\begin{itemize}
		\item Importación de datos de ventas desde Shopify: el sistema debe permitir la obtención de datos históricos y actualizados de ventas desde la plataforma Shopify, con el objetivo de utilizarlos en el proceso de entrenamiento del modelo de predicción de demanda.
		\item Importación de datos de inversión publicitaria desde Triple Whale: el sistema debe permitir la incorporación de datos de inversión publicitaria provenientes de la plataforma Triple Whale, con el propósito de integrar esta variable en el análisis y modelado de ventas.
		\item Consolidación de datos provenientes de Shopify y Triple Whale: el sistema debe unificar la información de ventas y de inversión publicitaria en un único conjunto de datos, con el fin de preparar una base consistente para el entrenamiento del modelo de predicción.
		\item Desarrollo y entrenamiento del modelo predictivo: el sistema debe implementar y entrenar un modelo de inteligencia artificial capaz de generar predicciones de ventas precisas sobre los datos históricos consolidados.
		\item Visualización de predicciones de ventas en Inventory Tracker: el sistema debe presentar las predicciones de ventas generadas por el modelo dentro de la plataforma Inventory Tracker, integradas con los datos históricos para facilitar el análisis y la planificación de producción.
		\item Descarga de predicciones de ventas en formato CSV: el sistema debe permitir la exportación de las predicciones de ventas a archivos CSV para su análisis externo o integración con otras herramientas de planificación.
	\end{itemize}




\section{Modelos de inteligencia artificial utilizados}
\label{sec:modelo-ia}

Para abordar el trabajo se evaluaron y aplicaron diversos modelos de inteligencia artificial, dado que el comportamiento de la demanda no responde a un único patrón fijo ni puede ser capturado adecuadamente por un solo tipo de modelo. También, se consideró seleccionar aquel que ofreciera el mejor equilibrio entre precisión, interpretabilidad y capacidad de generalización.

A continuación, se describen los tipos de modelos aplicados:

	\begin{itemize}
		\item Random Forest Regressor \citep{randomforestregressor}: se utilizó por su robustez frente a sobreajuste y su capacidad para capturar relaciones no lineales entre variables. Este modelo resulta especialmente útil cuando se integran múltiples fuentes de datos, como ventas históricas e inversión publicitaria.

		\item Gradient Boosting Machines \cite{friedman2001gbm} (XGBoost, LightGBM): se evaluaron por su alto rendimiento en competencias de ciencia de datos y su eficiencia computacional. Estos modelos permiten optimizar la función de pérdida de manera iterativa y mejorar la precisión en la predicción de series temporales con características externas.

	\item SARIMA (\textit{Seasonal Autoregressive Integrated Moving Average}): se aplicó para modelar la estacionalidad y tendencia presentes en los datos de ventas. SARIMA es especialmente efectivo cuando el comportamiento temporal es predominante y estable a lo largo del tiempo.

	\item \textit{Exponential Smoothing} (ETS) \citep{hyndman2008forecasting}: se consideró como alternativa para capturar patrones estacionales y de tendencia con un enfoque más intuitivo y menos paramétrico que SARIMA.

	\item \textit{Long Short-Term Memory} (LSTM): se implementaron redes LSTM debido a su habilidad para retener información a largo plazo y modelar dependencias temporales complejas. Este tipo de red es adecuado para series temporales con patrones no lineales y múltiples variables exógenas, como la inversión publicitaria por plataforma.

	\item GRU (\textit{Gated Recurrent Unit}) \citep{cho2014gru}: se evaluó como una variante más simple y computacionalmente eficiente de las LSTM, útil cuando los recursos de entrenamiento son limitados. Además, mantienen la capacidad esencial de capturar dependencias temporales a largo plazo y manejar el problema del gradiente \textit{vanishing}, que en ocasiones demuestra un desempeño comparable al de las LSTM.

	\item \textit{Temporal Fusion Transformer} (TFT): Se implementó este modelo de vanguardia por su capacidad de atención interpretable, que permite identificar qué variables exógenas y qué períodos históricos influyen en cada predicción. El TFT resulta particularmente adecuado para el contexto de Latech, donde es crucial entender el impacto de las inversiones publicitarias en diferentes horizontes temporales.

	\item Enfoque híbrido: se exploró la combinación de modelos clásicos (como SARIMA) con redes neuronales (como LSTM o TFT), con el fin de aprovechar la capacidad de los primeros para modelar componentes estacionales y de tendencia, y la flexibilidad de las segundas para capturar relaciones no lineales y efectos de variables externas.

	\item Prophet: se incluyó el modelo Prophet, desarrollado por Meta \citep{meta}, por su facilidad de uso y capacidad para manejar estacionalidades múltiples, días festivos y cambios en la tendencia. Este modelo resultó de interés para validar la estructura temporal de los datos antes de aplicar modelos más complejos.
	\end{itemize}

Cada uno de estos modelos fue entrenado y validado utilizando el conjunto de datos consolidado de ventas e inversión publicitaria, y se compararon mediante métricas como MAE (error absoluto medio), RMSE (raíz del error cuadrático medio) y MAPE (error porcentual absoluto medio), con el fin de seleccionar el que mejor se adaptara a las necesidades de pronóstico de Latech.


\section{Tratamiento de datos (herramientas)}

En este a sección se describen los procesos y herramientas utilizadas para la limpieza, transformación y análisis de los datos, así como los recursos externos desarrollados por terceros que resultaron fundamentales para la obtención y procesamiento de la información.




\subsection{Bibliotecas para procesamiento y análisis de datos}
\begin{itemize}
	\item Pandas \citep{pandas}: permite manipular y transformar datos tabulares, incluidas operaciones de filtrado, agrupamiento y combinación de datasets.

	\item NumPy \citep{numpy}: ofrece operaciones numéricas y manejo eficiente de arreglos multidimensionales.

	\item Scikit-learn \citep{sklearn}: empleada para el preprocesamiento de datos (escalado, codificación de variables categóricas) y la implementación de modelos clásicos de \textit{machine learning}.
	
	\item Pytorch \citep{pytorch}: framework que permite definir arquitecturas personalizadas, calcular gradientes automáticamente y realizar entrenamientos acelerados mediante GPU. Su integración con bibliotecas como NumPy, Pandas y Matplotlib facilita incorporarlo a flujos de trabajo existentes y explorar modelos predictivos más sofisticados que los basados únicamente en series temporales clásicas.

\end{itemize}


\subsection{Bibliotecas para visualización}

\begin{itemize}
		\item Matplotlib \citep{matplotlib} y Seaborn \citep{seaborn}: permiten generar gráficos estáticos que facilitan la identificación de patrones, tendencias y valores atípicos.
		
		

\item Plotly \citep{plotly}: utilizado para la creación de visualizaciones interactivas dentro de los notebooks.
\end{itemize}


\subsection{Plataformas y APIs externas}
\begin{itemize}
	\item Shopify API \citep{shopify}: proporciona acceso automatizado a datos históricos de ventas, productos y transacciones.
	
	\item Triple Whale API \citep{triplewhale}: ofrece datos de inversión publicitaria desglosados por plataforma y campaña.
	
	\item PostgreSQL \citep{postgresql_docs}: funciona como sistema de gestión de bases de datos destinado al almacenamiento estructurado de los datos consolidados.

	\item Cron \citep{cron_docs}: permite ejecutar tareas programadas a horas, fechas o intervalos fijos periódicos para automatización de \textit{pipelines}.
	
	\item AWS S3 \citep{aws_s3}: ofrece un servicio de almacenamiento de objetos en la nube de AWS que con alta escalabilidad, disponibilidad, durabilidad y seguridad necesarios para guardar los modelos y datos intermedios.
	
	\item Git \citep{git_pro}: sistema de control de versiones distribuido que mantiene un registro histórico de cambios en archivos de código, configuraciones y documentación, esencial para el desarrollo colaborativo y la reproducibilidad de experimentos.
\end{itemize}


\subsection{Exposición de servicios}
Para exponer endpoints que permiten consumir resultados, ejecutar procesos o servir modelos se utiliza FastAPI. Este framework destaca por su alto rendimiento y por su arquitectura asíncrona basada en ASGI \citep{asgi}.

Las características más relevantes para este proyecto son:
\begin{itemize}
	\item Alto rendimiento: maneja múltiples solicitudes de manera eficiente gracias a su soporte nativo para operaciones asíncronas.
	\item Validación y documentación automática: incorpora Pydantic  \citep{pydantic} para validar estructuras de entrada y genera documentación OpenAPI/Swagger \citep{swagger} sin requerir configuraciones adicionales.
	\item Flexibilidad: permite definir endpoints destinados al consumo de modelos, consultas filtradas, refresco de datos o ejecución de procesos administrativos.
\end{itemize}


\subsection{Contenerización}
\begin{itemize}
	\item Docker \citep{docker}: permite definir entornos de ejecución consistentes para los servicios del proyecto. Sus contenedores aseguran reproducibilidad, aislamiento y portabilidad entre distintos entornos.
	
	\item Docker compose \citep{docker_compose}: herramienta que simplifica la orquestación de múltiples contenedores Docker mediante archivos de configuración en formato YAML. Permite la definición y ejecución de aplicaciones multi-servicio de manera coordinada, lo que facilita la puesta en marcha de entornos complejos que incluyen la API de FastAPI, la base de datos PostgreSQL y servicios auxiliares de forma integrada. Docker Compose maneja automáticamente las dependencias entre servicios, las redes virtuales y los volúmenes de persistencia.
\end{itemize}

\subsection{Orquestación y pipelines}
\begin{itemize}
	\item Metaflow \citep{metaflow}: se utiliza como herramienta principal para estructurar y ejecutar pipelines de datos y experimentos, con registro automático de artefactos y trazabilidad.

	\item Apache Airflow \citep{airflow}: framework de orquestación que define flujos de trabajo como DAGs (\textit{Directed Acyclic Graphs}) \citep{airflow_dags} y permite programar ejecuciones periódicas, manejar dependencias entre tareas y monitorizar el estado de los procesos de datos.
	
\end{itemize}
