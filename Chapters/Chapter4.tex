% Chapter Template

\chapter{Ensayos y resultados} % Main chapter title

\label{Chapter4} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}
Todos los capítulos deben comenzar con un breve párrafo introductorio que indique cuál es el contenido que se encontrará al leerlo.  La redacción sobre el contenido de la memoria debe hacerse en presente y todo lo referido al proyecto en pasado, siempre de modo impersonal.

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Ensayos de modelos}
\label{sec:pruebasHW}

La idea de esta sección es explicar cómo se hicieron los ensayos, qué resultados se obtuvieron y analizarlos.

\section{Desempeño del modelo}
\section{Desempeño de la predicción}
\section{Comparación de algoritmos}
\section{Validación de cumplimiento de requerimientos}

A continuación se detalla la descripción técnica y alcance correspondientes a cada uno de los requerimientos especificados anteriormente.


\subsubsection{Requerimiento funcional 1}

Para cumplir este requerimiento, se desarrolló un módulo de integración que:

\begin{itemize}
    \item Establece conexión con la API de Shopify mediante credenciales \textit{API Key} y \textit{Access Token}.
    \item Obtiene y procesa los siguientes atributos mínimos: fecha de compra, identificador de producto, cantidad vendida, precio unitario y canal de venta.
    \item Permite especificar intervalos de fechas para la descarga de datos.
    \item Almacena los datos importados en una base de datos PostgreSQL destinada a su posterior uso en el modelado.
    \item Implementa validaciones y manejo de errores en caso de fallos de autenticación, pérdida de conexión o respuestas vacías.
    \item Registra la trazabilidad del proceso de importación mediante un archivo de log, incluyendo volumen de datos procesados y errores detectados.
    \item Verifica duplicidades para evitar el doble procesamiento de registros previamente importados.
\end{itemize}

\subsubsection{Requerimiento funcional 2}

Para satisfacer este requerimiento se desarrolló un módulo de integración que:

\begin{itemize}
    \item Establece conexión con la API de Triple Whale con una clave de acceso provista por la organización.
    \item Recupera la información de inversión por fecha, campaña y plataforma publicitaria.
    \item Permite seleccionar y configurar el intervalo temporal de los datos a importar.
    \item Almacena los datos obtenidos en una base de datos PostgreSQL, manteniendo una estructura consistente con el modelo de análisis utilizado.
    \item Implementa mecanismos de manejo de errores ante fallos de autenticación, interrupciones en la conexión o respuestas inválidas provenientes de la API.
    \item Registra el detalle del proceso mediante un archivo de log, incluyendo la cantidad de registros importados y los errores detectados durante la ejecución.
    \item Permite ejecutar el proceso de importación tanto de forma manual como programada mediante tareas periódicas.
    \item Incluye validaciones para evitar la duplicación de registros en caso de importaciones sobre el mismo rango de fechas.
\end{itemize}



\subsubsection{Requerimiento funcional 3}

Para satisfacer este requerimiento se desarrolló un proceso de consolidación que integra los datos provenientes de Shopify (ventas) y Triple Whale (inversión publicitaria), y garantiza la correcta correspondencia temporal entre ambas fuentes. En particular:

\begin{itemize}
    \item Se generó un único \textit{dataset} en el que las ventas diarias y la inversión total diaria se vinculan a través del campo de fecha.
    \item Se realizó el alineamiento temporal de las fechas, y se tuvo en cuenta posibles diferencias de zona horaria entre ambas plataformas.
    \item En los casos en que alguna de las fuentes no presentaba información para una fecha determinada, se mantuvo el registro correspondiente utilizando valores nulos para evitar la pérdida de información histórica.
    \item El conjunto de datos final incluye las siguientes columnas: \texttt{fecha},  \texttt{ventas\_diarias}, \texttt{inversion\_diaria} y \texttt{plataforma}.
    \item Se aplicaron validaciones para detectar y corregir duplicados, inconsistencias o formatos inválidos en los registros procesados.
    \item El proceso de consolidación se diseñó de manera automatizada y reproducible para su ejecución periódica sin intervención manual.
    \item El \textit{dataset} resultante se almacenó en un formato estructurado apto para ser consumido directamente en las etapas de modelado y entrenamiento.
    \item Se registraron las operaciones del proceso mediante archivos de log, incluyendo la cantidad total de registros consolidados y cualquier incidencia detectada durante la ejecución.
\end{itemize}



\subsubsection{Requerimiento funcional 4}


Para cumplir con este requerimiento se implementó un proceso de modelado y entrenamiento estructurado que incluyó las siguientes etapas:

\begin{itemize}
    \item Se empleó el \textit{dataset} consolidado, compuesto por variables de ventas y de inversión publicitaria, como fuente principal para el entrenamiento del modelo.
    \item El conjunto de datos fue dividido en subconjuntos de entrenamiento y validación, siguiendo una proporción aproximada de 80\% para entrenamiento y 20\% para validación.
    \item Se evaluaron distintos modelos de predicción utilizando métricas de error adecuadas para series temporales, tales como MAE, RMSE y MAPE, con el fin de comparar su desempeño.
    \item Se seleccionó como modelo final aquel que presentó el mejor desempeño sobre el conjunto de validación.
    \item El modelo entrenado quedó habilitado para generar predicciones futuras a partir de valores recientes de entrada.
    \item Se garantizó la reproducibilidad del proceso mediante la definición de un \textit{pipeline} de entrenamiento que permite repetir la ejecución bajo las mismas configuraciones.
    \item El modelo seleccionado y sus parámetros finales fueron almacenados para su posterior uso dentro del sistema Inventory Tracker.
    \item Se documentó el proceso de entrenamiento de manera detallada, incluyendo la selección de variables, las técnicas de preprocesamiento aplicadas y los métodos de evaluación utilizados.
    \item Se realizó una validación visual comparando las predicciones generadas frente a los valores reales observados, mediante gráficos que permitieron analizar la coherencia del modelo respecto a la dinámica histórica de las ventas.
\end{itemize}


\subsubsection{Requerimiento funcional 5}

Para cumplir con este requerimiento se implementaron las siguientes funcionalidades:

\begin{itemize}
    \item Las predicciones de ventas se muestran en la interfaz de Inventory Tracker junto a las fechas correspondientes.
    \item La visualización integra tanto las predicciones como los datos históricos de ventas para comparaciones temporales y análisis de tendencias.
    \item La información se presenta de manera clara y entendible, utilizando gráficos de líneas y tablas según corresponda.
    \item Los usuarios pueden seleccionar rangos de fechas específicos para observar predicciones detalladas de periodos concretos.
    \item Las predicciones se actualizan automáticamente al entrenarse un nuevo modelo o al actualizarse los datos de entrada y se garantiza que la información refleje siempre el estado más reciente.
    \item La interfaz muestra un indicador visual de la fecha y hora de generación de la última predicción.
    \item Se distingue claramente entre valores predichos y datos históricos que evitan confusiones en el análisis.
    \item La integración de las predicciones no afecta la funcionalidad ni el rendimiento general de la interfaz del módulo Inventory Tracker.
    \item El rendimiento de la interfaz se mantiene aceptable incluso cuando se manejan conjuntos de datos de gran tamaño.
\end{itemize}


\subsubsection{Requerimiento funcional 6}


Para cumplir con este requerimiento se implementaron las siguientes funcionalidades:

\begin{itemize}
    \item Se incorporó un botón o enlace claramente identificado que permite la descarga de las predicciones en formato CSV.
    \item El archivo CSV generado incluye las fechas y los valores de predicción correspondientes.
    \item De manera opcional, el archivo puede incluir los datos históricos de ventas para permitir la comparación con las predicciones.
    \item El formato del CSV es compatible con herramientas comunes de análisis, con valores separados por comas y codificación UTF-8.
    \item El nombre del archivo contiene la fecha y hora de generación que lo identifica.
    \item La descarga se realiza correctamente en navegadores modernos, incluyendo Chrome y Firefox.
    \item El contenido del archivo refleja exactamente la información presentada en la interfaz del módulo de predicción.
    \item Si aún no existen predicciones generadas, el botón de descarga se muestra deshabilitado.
\end{itemize}

\subsubsection{Requerimiento funcional 7}

Para cumplir con este requerimiento se implementaron las siguientes funcionalidades:

\begin{itemize}
    \item El sistema verifica automáticamente la integridad del \textit{dataset} antes de cada entrenamiento del modelo.
    \item Si se detectan datos faltantes críticos como fechas sin registros de ventas o de inversión publicitaria se genera una alerta.
    \item La alerta incluye información detallada sobre el tipo de dato faltante (ventas, inversión, o ambos), el rango de fechas afectado y el nivel de severidad del impacto estimado.
    \item La alerta se muestra de forma visible dentro del sistema para que los responsables puedan tomar conocimiento inmediato.
    \item Se envía una notificación por correo electrónico al responsable del sistema, en caso de estar configurada esta opción.
    \item El sistema no bloquea el entrenamiento del modelo, pero advierte que la precisión de las predicciones puede verse afectada.
    \item La alerta desaparece únicamente cuando los datos faltantes son completados o cuando se marca como revisada manualmente.
    \item Se registra un log de todas las alertas emitidas, accesible desde una sección de administración o monitoreo del sistema.
\end{itemize}

