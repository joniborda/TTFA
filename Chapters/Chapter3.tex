\chapter{Diseño e implementación} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% parámetros para configurar el formato del código en los entornos lstlisting
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  %escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  %extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  %frame=single,	                % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=[ANSI]C,                % the language of the code
  %otherkeywords={*,...},           % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname,                  % show the filename of files included with \lstinputlisting; also try caption instead of title
  morecomment=[s]{/*}{*/}
}

En este capítulo se describen los criterios utilizados para la construcción del desarrollo y la arquitectura definida para la solución. Se presenta la estructura general del sistema, las decisiones de diseño adoptadas y los componentes que permiten integrar fuentes de datos externas, procesarlas y generar pronósticos confiables de ventas.

\section{Arquitectura del sistema}

La arquitectura diseñada para el sistema de predicción de ventas se basa en un flujo de procesamiento secuencial y automatizado que conecta las fuentes de datos externas con el entorno interno del cliente. El diseño se orientó por tres criterios principales:

\begin{itemize}


\item Centralización y coherencia de datos. Se priorizó la unificación de múltiples fuentes (ventas, comportamiento de usuarios, inversión en publicidad).

\item Escalabilidad operativa que permita que el sistema integre nuevos canales, nuevas métricas o nuevos modelos sin alterar la estructura existente.

\item Procesos reproducibles y auditables para asegurar que cada etapa sea trazable y que el sistema pueda ser mantenido o ampliado por equipos técnicos futuros.

\end{itemize}

El sistema se organiza en una arquitectura por capas que separa claramente las responsabilidades, desde la obtención de los datos hasta la entrega del pronóstico al usuario final. Estas capas son:

\begin{enumerate}

\item Capa de recolección de datos (Data Ingestion Layer).

\item Capa de procesamiento y transformación (ETL).

\item Capa de almacenamiento estructurado (Data Storage Layer).

\item Capa de modelado predictivo (Modeling Layer).

\item Capa de integración operativa mediante microservicios.

\item Capa de visualización y consumo dentro de Inventory Tracker.

\end{enumerate}

Cada una de estas capas se comunica de manera acotada mediante APIs internas o a través de la base de datos para garantizar un bajo acoplamiento.


\subsection{Capa de recolección de datos}

Esta capa se encarga de conectarse periódicamente a APIs externas para obtener la información necesaria para entrenar y actualizar los modelos. Las dos fuentes externas son:
\begin{itemize}
	\item Shopify: funciona como el pilar central de la información comercial del sistema. A través de su API es posible acceder a los históricos de ventas, que incluye detalles como estado, valor total y descuentos. Además, cada orden incluye etiquetas que permiten distinguir si la compra corresponde a un cliente nuevo o recurrente, lo que posibilita analizar el comportamiento de los usuarios a lo largo del tiempo. Esta distinción es un insumo clave para caracterizar patrones de fidelidad, frecuencia de compra y recurrencia. A partir de la combinación de órdenes, etiquetas de comportamiento y series temporales de ventas, se logra reconstruir el ciclo completo de los clientes y diferenciar entre la demanda estable generada por suscriptores activos y la demanda variable asociada a compras espontáneas o impulsadas por campañas de marketing.

	\item Triple Whale: una plataforma que centraliza la información de inversión publicitaria proveniente de TikTok Ads \citep{tiktokAds}, Instagram Ads \citep{InstagramAds}, Facebook Ads \citep{FacebookAds} y Google Ads \citep{GoogleAds}. En este caso, su API no provee datos desagregados de campañas individuales, sino únicamente el monto diario total invertido en publicidad considerando todas las campañas activas. A pesar de esta limitación, esta información resulta suficiente para analizar la relación entre los niveles diarios de inversión publicitaria y las variaciones observadas en las ventas. De esta forma, el gasto diario consolidado funciona como un indicador de la presión publicitaria ejercida en cada jornada, lo que permite estudiar su impacto en el comportamiento de compra y en la demanda general del sistema.
\end{itemize}

Ambas integraciones están preparadas para ejecutarse en un proceso automatizado que consulta las APIs diariamente y almacena la información en una base de datos PostgreSQL \citep{postgresql_docs} interna. Este proceso opera bajo un módulo ETL \citep{microsoft_etl} donde se llevan a cabo tareas de validación, normalización y estandarización para garantizar la coherencia entre fuentes heterogéneas.

La decisión de almacenar localmente los datos en PostgreSQL responde a tres objetivos principales:

\begin{itemize}
    \item Autonomía del sistema: aunque se dispone de acceso a las APIs de Shopify y Triple Whale, estas pueden presentar interrupciones temporales, límites de tasa de consulta (\textit{rate limits}) o cambios no anunciados en su estructura. Al mantener una copia local de los datos, el sistema de pronóstico continúa funcionando incluso durante caídas de las APIs externas, garantizando disponibilidad continua.

    \item Rendimiento y velocidad: consultar datos históricos desde una base de datos local es significativamente más rápido que realizar múltiples solicitudes HTTP a APIs externas, especialmente cuando se procesan grandes volúmenes de datos durante el entrenamiento de modelos o la generación de informes.

    \item Desacoplamiento y control de datos: al poseer una copia estructurada de los datos, se evita la dependencia directa de la disponibilidad y formato de las APIs externas. Esto permite realizar transformaciones, enriquecimientos y agregaciones previas que optimizan los procesos posteriores de modelado y análisis.
\end{itemize}

De esta manera, PostgreSQL actúa como una capa de abstracción y resiliencia, que permite que el sistema opere de manera eficiente, predecible y con menor latencia, sin comprometer su funcionalidad ante eventuales fallos externos.

\subsection{Capa de procesamiento y transformación}

Una vez obtenidos los datos desde las APIs externas, se realiza un proceso de transformación que normaliza y estructura la información. Las principales tareas incluyen:

\begin{itemize}

\item Limpieza de campos inconsistentes o incompletos.

\item Conversión de formatos de fechas, monedas y valores numéricos.

\item Enriquecimiento de datos combinando ventas, comportamiento de usuarios y gasto publicitario.

\item Integración de información transaccional con métricas de suscripción.

\item Control de duplicados y estandarización de claves primarias.

\item Creación de agregaciones temporales diarias.
 
\item Creación de variables derivadas de los datos como medias móviles o ratios de inversión.

\item Identificación de picos de campañas.

\item Señalización de eventos especiales tales como: promociones, feriados, lanzamientos.

\item Integración del estado de suscripciones activas e inactivas.

\end{itemize}

Este proceso de transformación se implementó mediante DAGs (\textit{Directed Acyclic Graphs}) en Apache Airflow, lo que permite orquestar de manera programada, reproducible y monitorizable cada etapa del pipeline de datos.

Los datos intermedios y finales de este proceso se almacenan temporalmente en disco, en un formato estructurado y accesible llamado Parquet, para permitir su reutilización en ejecuciones posteriores sin necesidad de reprocesar desde cero. Este enfoque no solo optimiza el uso de recursos, sino que también facilita la depuración y auditoría de los datos generados en cada etapa.

Dado que Shopify y Triple Whale estructuran sus datos de manera diferente, se construyó un esquema interno unificado que respeta la granularidad diaria necesaria para los modelos de predicción. Este esquema queda materializado en una tabla consolidada dentro de PostgreSQL, lista para su consumo por el módulo de modelado.

La implementación mediante Airflow permite además:

\begin{itemize}
\item Ejecución programada o bajo demanda de los flujos de transformación.
\item Reejecución selectiva de tareas fallidas sin afectar etapas exitosas.
\item Monitoreo visual del estado del pipeline y alertas ante incidencias.
\item Versionado y mantenimiento independiente de la lógica de transformación.
\end{itemize}

Este módulo permite encapsular toda la lógica de preparación del dataset, manteniéndolo desacoplado del motor de predicción para que se pueda actualizar fácilmente sin impactar en la generación de pronósticos.


\subsection{Capa de almacenamiento estructurado}

Luego de su procesamiento, los datos se almacenan en una base de datos PostgreSQL diseñada específicamente para consultas analíticas y para servir como fuente unificada de información durante el entrenamiento y evaluación de los modelos. Dentro de esta capa se construye la tabla principal denominada \textit{orders}, en la que se almacena la información proveniente de Shopify ya depurada y enriquecida. Esta tabla contiene las columnas fundamentales para caracterizar el comportamiento de compra de los usuarios:
\begin{itemize}
	\item \textit{created}: registra la fecha de creación de cada orden.
	\item \textit{totalPrice}: correspondiente al monto total pagado.
	\item \textit{customerId}: identifica al cliente.
	\item \textit{lineItems}: donde se detalla cada producto incluido en la compra junto con sus cantidades.
	\item \textit{channel}: indica el canal por el cual ingresó la orden.
	\item \textit{tags}: permite distinguir si la compra corresponde a un cliente nuevo o recurrente mediante etiquetas provistas por Shopify.
	\item \textit{orderNumberForCustomer}: señala el número de orden que representa dentro del historial del cliente (por ejemplo, un valor igual a 1 implica un cliente nuevo).
	\item \textit{diffWeeksFromFirstPurchase}: expresa la cantidad de semanas transcurridas desde la primera compra del usuario, lo que permite analizar patrones de frecuencia y retención.
	
\end{itemize}

A partir de esta tabla estructurada, se generan columnas derivadas mediante procesos de enriquecimiento en Python, con el objetivo de facilitar el análisis y preparar los datos para su uso en modelos predictivos. Entre estas variables se incluyen:
\begin{itemize}
	\item \textit{created\_weekday}: identifica el día de la semana de cada orden.
	\item \textit{created\_month}: permite estudiar estacionalidades mensuales. 
	\item \textit{unique\_customers}: calcula la cantidad de clientes distintos por día.
	\item \textit{new\_customers}: contabiliza cuántos usuarios realizaron su primera compra dentro de cada fecha.
	\item \textit{returning\_customers}: contabiliza cuántos usuarios realizaron más de una compra. 
\end{itemize}

Estas nuevas columnas permiten construir vistas agregadas y analizar la evolución del comportamiento de los clientes en el tiempo.

Finalmente, otra tabla denominada \textit{forecast} consolida la información diaria extraida de TripleWhale. Esta tabla incorpora, entre otras variables, el monto de inversión publicitaria bajo la columna \textit{ad\_spend}. La tabla contiene además otros campos utilizados por el sistema Inventory Tracker, derivados de los datos ya existentes. Sin embargo, dichas columnas no forman parte del alcance del presente trabajo, ya que responden a necesidades operativas específicas del cliente y no intervienen en el proceso de modelado.

Este diseño facilita tanto el acceso para entrenamiento de modelos como la consulta rápida desde Inventory Tracker. En la figura \ref{fig:der} se muestran las dos tablas principales del trabajo con las columnas que se tomaron en cuenta para este proceso de almacenamiento.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{./Figures/der.png}
    \caption{Diagrama Entidad-Relación. Muestra las dos tablas principales.}
    \label{fig:der}
\end{figure}


\subsection{Capa de modelado predictivo}

Esta capa es responsable del entrenamiento, evaluación y selección de los modelos de pronóstico de ventas. Se implementa un enfoque comparativo donde se ejecutan simultáneamente los modelos mencionados en el capítulo \ref{sec:modelo-ia} (SARIMA, Exponential Smoothing, Random Forest, XGBoost, LightGBM, LSTM y Prophet), con el objetivo de identificar el que ofrezca el mejor desempeño predictivo según las métricas definidas.

El proceso de modelado sigue las siguientes etapas:

\begin{enumerate}
	\item Entrenamiento múltiple: todos los modelos son entrenados utilizando el mismo conjunto de datos históricos consolidado y enriquecido.
	\item Evaluación y métricas: cada modelo es evaluado con la métrica RMSE para penalizar errores grandes que impactan significativamente en la planificación financiera y de producción.

	\item Selección automática: el modelo que obtiene el menor valor de RMSE en el conjunto de validación es seleccionado automáticamente para su uso en producción.
	\item Persistencia del modelo: una vez seleccionado, el modelo ganador es serializado y almacenado en un registro de modelos donde queda disponible para su uso en las predicciones futuras.

	\item Exposición mediante API: el modelo guardado es integrado en un endpoint REST que forma parte del sistema de microservicios para que el sistema Inventory Tracker consulte las predicciones de ventas en tiempo real.
\end{enumerate}








Este diseño no solo asegura que siempre se esté utilizando el modelo más preciso disponible, sino que también facilita la actualización periódica del mismo mediante reentrenamientos programados. Con cada actualización se mantiene la capacidad predictiva del sistema ante cambios en el comportamiento de las ventas o en las dinámicas de mercado.




\subsection{Capa de integración operativa con microservicios}

La arquitectura del sistema se basa en un enfoque de microservicios, donde cada funcionalidad clave se encapsula en un servicio independiente, desplegado y mantenido de forma aislada. Los microservicios implementados son los siguientes:

\begin{itemize}
	\item Servicio de ingestión (cronjobs + conectores API).
	\item Servicio de ETL.
	\item Servicio de predicción.
	\item Servicio de API interna para Inventory Tracker.
\end{itemize}

La comunicación entre estos servicios se realiza mediante endpoints REST internos para operaciones síncronas y, cuando se requiere procesamiento asincrónico o desacoplamiento temporal, a través de colas de mensajería (por ejemplo, con RabbitMQ \citep{rabbitmq} o Redis \citep{redis}). Este diseño permite que cada servicio evolucione y escale de forma independiente, sin afectar al resto del sistema.


Los principales beneficios de esta capa de microservicios son:

\begin{itemize}
	\item Actualizaciones incrementales: cada servicio puede ser actualizado, modificado o reemplazado sin impactar en el funcionamiento de los demás, lo que facilita la incorporación de mejoras o nuevas fuentes de datos.

	\item Escalabilidad selectiva: los módulos con mayor carga computacional, como el servicio de predicción o de ingestión, pueden escalarse horizontalmente de forma independiente, optimizando el uso de recursos.

	\item Aislamiento de fallos: un fallo en un servicio (por ejemplo, la indisponibilidad temporal de una API externa) no propaga su error al resto del sistema, gracias al desacoplamiento y a los mecanismos de tolerancia a fallos implementados.

	\item Facilidad de mantenimiento y despliegue: cada servicio cuenta con su propio repositorio, configuración y ciclo de vida, lo que simplifica las tareas de desarrollo, pruebas y despliegue continuo.
	
\end{itemize}


\subsection{Capa de visualización y consumo dentro de Inventory Tracker}

Las predicciones de ventas finales se integran en el sistema Inventory Tracker mediante una interfaz intuitiva, la cual permite:

\begin{itemize}
	\item Solicitar pronósticos para distintos horizontes temporales.
	\item Visualizar ventas históricas junto con proyecciones.
	\item Combinar la predicción con datos operativos del cliente como: stock, producción, etc.
\end{itemize}

Este componente cierra el ciclo de valor al convertir la información generada por el sistema en una herramienta de apoyo para la gestión diaria.


\subsection{Justificación global de la arquitectura}

La arquitectura fue diseñada bajo los siguientes principios:

\begin{itemize}
	\item Modularidad: permite reemplazar o actualizar partes sin reestructurar toda la solución.

	\item Escalabilidad: preparada para el aumento del volumen de datos y la posible integración de nuevas plataformas.

	\item Trazabilidad: indispensable para un sistema basado en machine learning.

	\item Flexibilidad: permite combinar modelos clásicos, modelos de machine learning y eventualmente servicios administrados en la nube.
\end{itemize}


\section{Automatización mediante tareas programadas}

Para garantizar el funcionamiento continuo y automático del sistema, se diseñó e implementó un conjunto de tareas programadas mediante cronjobs que se ejecutan de manera secuencial y periódica, sin requerir intervención manual. Este flujo automatizado permite mantener la información del sistema actualizada de forma confiable y oportuna. Las principales tareas programadas incluyen:

\begin{itemize}
    \item Extracción diaria de datos: se ejecuta una conexión automatizada a las APIs de Shopify y Triple Whale para obtener los datos más recientes de ventas e inversión publicitaria. Este proceso garantiza que el sistema opere con información actualizada y evita retrasos en la disponibilidad de datos.
    
    \item Actualización del dataset consolidado: los nuevos datos se procesan, limpian y unifican con el historial existente. Este paso genera un dataset integral y consistente que sirve como base para el entrenamiento del modelo y la generación de pronósticos.
    
    \item Reentrenamiento periódico del modelo: con una frecuencia configurada (por ejemplo, semanal o mensual), el sistema ejecuta automáticamente el proceso de reentrenamiento del modelo de pronóstico utilizando el dataset actualizado. Esto permite que el modelo se adapte a cambios en las tendencias de ventas, comportamiento del mercado o nuevos patrones estacionales.
    
    \item Generación de nuevas predicciones: después del reentrenamiento, el modelo genera pronósticos actualizados para los períodos futuros definidos. Estas predicciones reemplazan a las versiones anteriores y se almacenan en la base de datos para su consulta.
    
    \item Invalidación y regeneración de caché en Inventory Tracker: Para asegurar que los usuarios accedan siempre a la información más reciente, el sistema invalida automáticamente la caché del Inventory Tracker y la regenera con las nuevas predicciones y datos consolidados. Esto elimina posibles desfases entre los datos mostrados y la realidad del sistema.
\end{itemize}

Adicionalmente, se implementaron mecanismos de monitoreo y registro (\textit{logging}) para cada tarea programada, lo que permite auditar el correcto funcionamiento del flujo automatizado y detectar posibles fallos de manera temprana. 

Esta automatización integral asegura que el cliente disponga permanentemente de información actualizada, confiable y lista para la toma de decisiones, mientras se reduce significativamente el riesgo de errores humanos y se optimiza el mantenimiento operativo del sistema.


\section{Análisis de datos}
\section{Desarrollo de modelos}
\section{Desarrollo de un framework modular}
\section{Despliegue con microservicios e integración con bases de datos}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{./Figures/diagBloques.png}
	\caption{Flujo general del sistema para generar el pronóstico de ventas.}

	\label{fig:texmaker}
\end{figure}
